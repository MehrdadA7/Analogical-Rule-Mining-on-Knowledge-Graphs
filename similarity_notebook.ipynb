{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Similarity-implementations\" data-toc-modified-id=\"Similarity-implementations-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Similarity implementations</a></span><ul class=\"toc-item\"><li><span><a href=\"#Subject-Similarity-\" data-toc-modified-id=\"Subject-Similarity--1.1\"><span class=\"toc-item-num\">1.1&nbsp;&nbsp;</span>Subject Similarity <a id=\"sub\"></a></a></span></li><li><span><a href=\"#OpenKE-based-similarities---\" data-toc-modified-id=\"OpenKE-based-similarities----1.2\"><span class=\"toc-item-num\">1.2&nbsp;&nbsp;</span>OpenKE based similarities  <a id=\"OpenKE\"> </a></a></span><ul class=\"toc-item\"><li><span><a href=\"#Cosine--\" data-toc-modified-id=\"Cosine---1.2.1\"><span class=\"toc-item-num\">1.2.1&nbsp;&nbsp;</span>Cosine  <a id=\"cos\"></a></a></span><ul class=\"toc-item\"><li><span><a href=\"#gen_all_cos()\" data-toc-modified-id=\"gen_all_cos()-1.2.1.1\"><span class=\"toc-item-num\">1.2.1.1&nbsp;&nbsp;</span>gen_all_cos()</a></span></li></ul></li><li><span><a href=\"#DistMult-Avg--\" data-toc-modified-id=\"DistMult-Avg---1.2.2\"><span class=\"toc-item-num\">1.2.2&nbsp;&nbsp;</span>DistMult Avg  <a id=\"avg\"></a></a></span></li></ul></li></ul></li><li><span><a href=\"#Append-module\" data-toc-modified-id=\"Append-module-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Append module</a></span></li><li><span><a href=\"#AMIE-and-Evaluations\" data-toc-modified-id=\"AMIE-and-Evaluations-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>AMIE and Evaluations</a></span><ul class=\"toc-item\"><li><span><a href=\"#Baseline-evaluation\" data-toc-modified-id=\"Baseline-evaluation-3.1\"><span class=\"toc-item-num\">3.1&nbsp;&nbsp;</span>Baseline evaluation</a></span></li><li><span><a href=\"#Enriched-KB-Evaluation\" data-toc-modified-id=\"Enriched-KB-Evaluation-3.2\"><span class=\"toc-item-num\">3.2&nbsp;&nbsp;</span>Enriched KB Evaluation</a></span></li></ul></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Similarity implementations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Important files:\n",
    "- `./OpenKE/benchmarks/FB15K/train2id.txt` : \\[int int int\\]. Used by OpenKE based models (cosine, DistMult Avg) to find embeddings and Subject Similarity to find similarities.\n",
    "- `./OpenKE/benchmarks/FB15K/entity2id.txt` : \\[/mid int\\].  \n",
    "    - Used for translating similarity df from previous step to a df with this structure: \\[ /mid /similar_to /mid \\] .\n",
    "    - Also used by Word2vec notebook.\n",
    "- `./FB15K/mid2name.tsv` : \\[ /mid word \\]. Used by word2vec notebook.\n",
    "- `./FB15K/train.txt` : \\[/mid /mid /mid\\]. Used by AMIE. Thus, enrichement must happen on this file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-17T21:46:25.921390Z",
     "start_time": "2020-05-17T21:46:20.558936Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mehrdadalvandipour/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/Users/mehrdadalvandipour/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/Users/mehrdadalvandipour/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:528: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/Users/mehrdadalvandipour/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:529: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/Users/mehrdadalvandipour/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:530: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/Users/mehrdadalvandipour/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:535: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "import itertools\n",
    "from itertools import combinations\n",
    "import pandas as pd\n",
    "import random\n",
    "\n",
    "from OpenKE import models,config\n",
    "import multiprocessing\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "import os\n",
    "\n",
    "\n",
    "\n",
    "SUBJ = True\n",
    "COS = True\n",
    "DIST_AVG = True\n",
    "\n",
    "\n",
    "SUBJ_SCORE = 0\n",
    "COS_THR = .8\n",
    "DIST_AVG_THR = -3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Subject Similarity <a id=\"sub\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The definition of similarity in this module is as follows:  \n",
    "\n",
    "Let $e_i$ be an entity in the KB. Associated to $e_i$ there's a set $S_{e_i}$ defined by:\n",
    "\n",
    "\n",
    "$$\n",
    "S_{e_i} = \\{ (r,e) | (e_i ,r,e) \\in KB \\}.\n",
    "$$\n",
    "\n",
    "It's the set of tuples $(r,e)$ such that $(e_i,r,e)$ is a triplet in the KB."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "We say $e_i$ is similar to $e_j$ if $S_{e_i} \\cap S_{e_j} \\neq \\emptyset$. Moreover, we define the similarity score between $e_i$ and $e_j$ by $sc(e_i,e_j) = | S_{e_i} \\cap S_{e_j} |$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-13T02:26:13.700435Z",
     "start_time": "2020-05-13T02:25:39.865839Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.2668290551811503%\n"
     ]
    }
   ],
   "source": [
    "# An implementation of the above is here:\n",
    "# \n",
    "# Read every line in train2id and store the sets S_{e_i} in a list.\n",
    "#\n",
    "if SUBJ:\n",
    "    ent_total = 14951    # From the first line of ent2id.txt\n",
    "    file = \"./OpenKE/benchmarks/FB15K/train2id.txt\"\n",
    "    s = [ [] for _ in range(ent_total)] # A list of lists(sets). Will contain all sets S_{e_i}.\n",
    "\n",
    "    f = open(file,'r')\n",
    "    num_lines = f.readline()    # First line of train2id is the number of triplets\n",
    "    for i in range(int(num_lines)):\n",
    "        l = f.readline()\n",
    "        l = l.split()\n",
    "        try:\n",
    "            s[int(l[0])] += [ ( int(l[1]) , int(l[2]) ) ]\n",
    "        except:\n",
    "            print(\"something went wrong at triple\" + str(i))\n",
    "            raise\n",
    "    f.close()\n",
    "    ####################################\n",
    "    # ent_total = ent_total // 50     # for speed boost in tests\n",
    "    tot = ent_total *(ent_total -1)/2\n",
    "    c = 0 # counter \n",
    "\n",
    "    h = []\n",
    "    t = []\n",
    "    sc = []\n",
    "    try:\n",
    "        for i,j in combinations(range(ent_total),2):\n",
    "            c +=1\n",
    "            score = len( set(s[i]) & set(s[j]) )\n",
    "            if score > SUBJ_SCORE:\n",
    "                h.append(i)\n",
    "                t.append(j)\n",
    "                sc.append(score)\n",
    "    except KeyboardInterrupt:\n",
    "        print(str(100*c/ tot) + \"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With this data frame in hand, we can play so many different games, i.e. filtering out low scores, etc. But at the end these must be appended to the training file in mid format. So each integer `id` must be translated to `/mid` an then appended to train.txt in the form `/mid /similar_to /mid`.\n",
    "\n",
    "For the appending operation we have defined another module that takes the filtered data frame as input. The filtered data frame must have only two columns named 'head' and 'tail'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-13T02:29:09.434283Z",
     "start_time": "2020-05-13T02:29:09.254291Z"
    }
   },
   "outputs": [],
   "source": [
    "if SUBJ:\n",
    "    d = {'head':h , 'tail':t, 'score': sc}\n",
    "    df = pd.DataFrame(data=d)\n",
    "\n",
    "    # df.loc[df['score'] == 2] # Nullius in verba\n",
    "    filt_sub_df = df.copy()\n",
    "    #filt_sub_df = df.loc[df['score'] > 2].copy() # More filtering if needed\n",
    "    filt_sub_df.drop(columns='score',inplace=True)\n",
    "    filt_sub_df # Nullius in verba again"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We write the function call for appending in the next cell in comments. So whenever needed you just need to uncomment and call create the train_enriched.txt in `./FB15K` folder. This will be later used by AMIE to mine rules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-13T00:47:01.737752Z",
     "start_time": "2020-05-13T00:14:36.325Z"
    }
   },
   "outputs": [],
   "source": [
    "# filt_sub_df #feed it to append module "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OpenKE based similarities  <a id=\"OpenKE\"> </a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have two similarity routine based on DistMult in OpenKE. One is cos_sim and the other is avg_dist_mult. For these to run and generate the similarity data frame, first we need to run the OpenKE model which learns the embeddings. This is the first task, done in the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-17T21:46:55.268143Z",
     "start_time": "2020-05-17T21:46:26.969711Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "WARNING: The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
      "For more information, please see:\n",
      "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
      "  * https://github.com/tensorflow/addons\n",
      "If you depend on functionality not listed there, please file an issue.\n",
      "\n",
      "WARNING:tensorflow:From /Users/mehrdadalvandipour/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From /Users/mehrdadalvandipour/git_dir/Analogical-Rule-Mining-on-Knowledge-Graphs/OpenKE/models/DistMult.py:12: calling reduce_sum_v1 (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "keep_dims is deprecated, use keepdims instead\n",
      "(4831, 1)\n",
      "WARNING:tensorflow:From /Users/mehrdadalvandipour/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Epoch: 0, loss: 107.23864191770554, time: 2.6291282176971436\n",
      "Epoch: 1, loss: 80.29576188325882, time: 1.8825621604919434\n",
      "Epoch: 2, loss: 58.39328280091286, time: 1.8455169200897217\n",
      "Epoch: 3, loss: 38.54874390363693, time: 1.850348949432373\n",
      "Epoch: 4, loss: 28.009576082229614, time: 1.8214657306671143\n"
     ]
    }
   ],
   "source": [
    "if COS or DIST_AVG:\n",
    "    con = config.Config()\n",
    "    con.set_in_path('./OpenKE/benchmarks/FB15K/')\n",
    "\n",
    "    con.set_test_link_prediction(True)\n",
    "    con.set_test_triple_classification(True)\n",
    "\n",
    "    con.set_work_threads(multiprocessing.cpu_count())\n",
    "\n",
    "    con.set_train_times(5)  # To set the data traversing rounds\n",
    "    con.set_nbatches(100)     # To split the training triples into several batches\n",
    "    con.set_alpha(0.1)        # To set the learning rate\n",
    "    con.set_dimension(100)    # To set the dimensions of the entities and relations at the same time\n",
    "    # con.set_margin(1)         # To set the margin for the loss function\n",
    "\n",
    "    con.set_bern(0)            # To set negative sampling algorithms, unif (bern = 0) or bern (bern = 1)\n",
    "    con.set_ent_neg_rate(1)   # For each positive triple, we construct rate negative triplentity\n",
    "    con.set_rel_neg_rate(0)\n",
    "\n",
    "    con.set_opt_method(\"Adagrad\") \n",
    "\n",
    "    con.set_export_files(\"./OpenKE/res/model.vec.tf\", 0)  # To set the export file of model paramters, every few rounds\n",
    "    con.set_out_files(\"./OpenKE/res/embedding.vec.json\") \n",
    "\n",
    "    con.init()\n",
    "\n",
    "    #Set the knowledge embedding model\n",
    "    con.set_model(models.DistMult)\n",
    "\n",
    "    con.run()\n",
    "\n",
    "\n",
    "    #con.test()\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-17T23:21:34.894283Z",
     "start_time": "2020-05-17T23:21:34.884228Z"
    }
   },
   "outputs": [],
   "source": [
    "embeddings = con.get_parameters('numpy')\n",
    "ents = embeddings['ent_embeddings'] # Table of all entity vectors\n",
    "ent_total = len(ents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next two sections(\"Cosine\" and \"DistMult Avg\"), use the embeddings in `ents` and `rels`. In fact \"Cosine\" only uses `ents`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cosine  <a id=\"cos\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vinay has written a few functions that I copy here. We only use the first three."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-13T02:31:10.638042Z",
     "start_time": "2020-05-13T02:31:10.626565Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def dot(x,y):\n",
    "    return np.sum(x * y) \n",
    "\n",
    "# Vector Magnitude\n",
    "def mag(x):\n",
    "    return np.sqrt(np.sum(x * x))\n",
    "\n",
    "# Cosine Similarity\n",
    "def cosine_similar_to(h,t,ent_embeddings = ents):\n",
    "    ent_h = ent_embeddings[h]\n",
    "    ent_t = ent_embeddings[t]\n",
    "    cos_sim = np.absolute(dot(ent_h,ent_t)) / (mag(ent_h) * mag(ent_t))\n",
    "    return(cos_sim)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-13T02:31:13.130948Z",
     "start_time": "2020-05-13T02:31:13.122697Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def rand_comb(n): \n",
    "    \"\"\" Return a generator(iterator) object that simulates random n choose 2.\n",
    "    \n",
    "    In other words, randomly select an entry from upper or lower half\n",
    "    of a symmetric matrix and return it.\n",
    "    You can also check the histogram to see the choice are not biased \n",
    "    toward upper/lower half.\n",
    "    \"\"\"\n",
    "    mem_set = set()\n",
    "    c = 0\n",
    "    while c < n*(n-1)/2:\n",
    "        while True:\n",
    "            i = random.randint(0,n-1)\n",
    "            j = random.randint(0,n-1)\n",
    "            while j==i:\n",
    "                j = random.randint(0,n-1)\n",
    "            if set([i,j]) not in mem_set:\n",
    "                break     \n",
    "        mem_set.add(frozenset([i,j]))\n",
    "        c+=1\n",
    "        yield (i,j)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want to generate the data frame partialy then you might want to randomize the process.\n",
    "`gen_all_cos()` has all the options and you can stop it whenever you want and get partial results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### gen_all_cos()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-13T02:31:16.126813Z",
     "start_time": "2020-05-13T02:31:16.102958Z"
    }
   },
   "outputs": [],
   "source": [
    "def gen_all_cos(rand=False):\n",
    "    \"\"\"Return a data frame containing head, tail, and score for similar tuples.\n",
    "    \n",
    "    Rand -- (default=False): chose tuples randomly.\n",
    "    Also prints the percentage of checked tuples. If interupted (KeyboardInterrupt)\n",
    "    it still returns the (incomplete) data frame.\n",
    "    \"\"\"\n",
    "    # Pros:\n",
    "    # 1) you can stop it anytime and get partial results\n",
    "    # 2) It attempts to exhaust all combinations in a random way. So the histogram of \n",
    "    # heads/tails is pretty uniform whenever you stop.\n",
    "    # Cons:\n",
    "    # 1) A little bit slower than the next method.\n",
    "    # 2) probbaly even slower when randomizing and near the end of the process.\n",
    "    tot = ent_total*(ent_total-1)/2\n",
    "    c=0\n",
    "    try:\n",
    "        head = []\n",
    "        tail = []\n",
    "        sc = []\n",
    "        if rand:\n",
    "            for i,j in rand_comb(ent_total):\n",
    "                if not(c%100000):\n",
    "                    print(100*c/tot) # print progress % every 100k iteration\n",
    "                c+=1\n",
    "                score = cosine_similar_to(i,j)\n",
    "                if score > COS_THR:\n",
    "                    head.append(i)\n",
    "                    tail.append(j)\n",
    "                    sc.append(score)\n",
    "        else:\n",
    "            for i,j in combinations(range(ent_total),2):\n",
    "                if not(c%100000):\n",
    "                    print(100*c/tot) # print progress % every 100k iteration\n",
    "                c+=1\n",
    "                score = cosine_similar_to(i,j)\n",
    "                if score > COS_THR:\n",
    "                    head.append(i)\n",
    "                    tail.append(j)\n",
    "                    sc.append(score)\n",
    "    except KeyboardInterrupt:\n",
    "        print('KeyboardInterrupt at ' + f'{100*c/tot:.3f} %')\n",
    "        \n",
    "        if len(head) > len(tail):\n",
    "            print(\"head popped!\")\n",
    "            head.pop()\n",
    "        elif len(head) < len(tail):\n",
    "            print(\"tail popped!\")\n",
    "            tail.pop()\n",
    "        \n",
    "        if len(sc) < len(head):\n",
    "            print(\"pop pop!\")\n",
    "            head.pop()\n",
    "            tail.pop()\n",
    "        \n",
    "        if not(len(sc) == len(head) == len(tail)):\n",
    "            print(\"\"\"len s,h,t still don't match :( Ain't \n",
    "                  possible but if happened, then last row contains NaNs \"\"\")\n",
    "            h = pd.DataFrame({'head':head})\n",
    "            t = pd.DataFrame({'tail':head})\n",
    "            s = pd.DataFrame({'sc':head})\n",
    "            new = pd.concat([h, t,s], axis=1)\n",
    "            return new\n",
    "        \n",
    "        print(\"head: \" + str(len(head)) + \" tail: \" + str(len(tail)) + \" sc: \" + str(len(sc)) )\n",
    "    \n",
    "    d = {'head':head , 'tail':tail, 'score': sc}\n",
    "    df = pd.DataFrame(data=d)\n",
    "    return df "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-13T02:32:16.718534Z",
     "start_time": "2020-05-13T02:31:54.334910Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n",
      "0.08947847248615265\n",
      "0.1789569449723053\n",
      "0.26843541745845795\n",
      "0.3579138899446106\n",
      "0.44739236243076325\n",
      "0.5368708349169159\n",
      "0.6263493074030685\n",
      "0.7158277798892212\n",
      "KeyboardInterrupt at 0.736 %\n",
      "head: 70 tail: 70 sc: 70\n"
     ]
    }
   ],
   "source": [
    "if COS:\n",
    "    df_cos_sim = gen_all_cos()\n",
    "    #filt_cos_df = df_cos_sim.loc[df_cos_sim['score'] > .85].copy() # more filtering if necc.\n",
    "    filt_cos_df = df_cos_sim.copy()\n",
    "    filt_cos_df.drop(columns='score',inplace=True)\n",
    "    print(filt_cos_df) # feed it to append module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-13T00:47:01.780585Z",
     "start_time": "2020-05-13T00:14:36.382Z"
    }
   },
   "outputs": [],
   "source": [
    "# pass to append module"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DistMult Avg  <a id=\"avg\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this method, for each pair $(e_i,e_j)$ of entities we calculate the DistMult loss $l_k$ of \n",
    "$(e_i, r_k ,e_j)$ for all $k$. Since the $l_k$ is actully loss value, we take those that are negative and average them to get the score value $Sc(e_i,e_j)$. Then this score value is used to decide wheather $e_i$ and $e_j$ are similar or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-17T23:20:45.645447Z",
     "start_time": "2020-05-17T23:20:45.638606Z"
    }
   },
   "outputs": [],
   "source": [
    "rels = embeddings['rel_embeddings'] # Get the relation embeddings from OpenKE\n",
    "rel_total = len(rels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we calculate the score for all pairs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-17T23:21:47.010794Z",
     "start_time": "2020-05-17T23:21:46.999437Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rem is 1.0\n",
      "Total steps that the next blocks gonna take: 22429.0\n"
     ]
    }
   ],
   "source": [
    "# To optimize the calculations and use GPU, we do it in chunks (batches)\n",
    "# Hrere you can choose a chunk size and then run the next cell.\n",
    "\n",
    "chunk_size =  ent_total//3  # choose a chunk size. ent_total//3 for instance worked on server\n",
    "\n",
    "tot = ent_total * (ent_total - 1)/2\n",
    "rem = tot % chunk_size\n",
    "if rem == 0:\n",
    "    step_total = tot // chunk_size\n",
    "else:\n",
    "    print('rem is ' + str(rem))\n",
    "    step_total = (tot // chunk_size) + 1\n",
    "\n",
    "print( 'Total steps that the next blocks gonna take: ' + str(step_total))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-17T23:26:33.640068Z",
     "start_time": "2020-05-17T23:22:13.904669Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 45.40415287017822\n",
      "2 46.16051506996155\n",
      "3 43.35315203666687\n",
      "4 52.59141182899475\n",
      "5 43.17501401901245\n",
      "6 53.83556580543518\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-29dd99e0c312>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0;31m#res = con.test_step(h.repeat(rel_total), t.repeat(rel_total), r_t).reshape(-1,rel_total)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m     \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcon\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtest_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mh\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrepeat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrel_total\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrepeat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrel_total\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mr_t\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m     \u001b[0mind\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mres\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0mdis_avg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdivide\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmultiply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mres\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mind\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mrel_total\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mrel_total\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m \u001b[0;36m1e-9\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/git_dir/Analogical-Rule-Mining-on-Knowledge-Graphs/OpenKE/config/Config.py\u001b[0m in \u001b[0;36mtest_step\u001b[0;34m(self, test_h, test_t, test_r)\u001b[0m\n\u001b[1;32m    314\u001b[0m                         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainModel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_r\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtest_r\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    315\u001b[0m \t\t}\n\u001b[0;32m--> 316\u001b[0;31m                 \u001b[0mpredict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainModel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    317\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mpredict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    318\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    927\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    928\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 929\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    930\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    931\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1150\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1151\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1152\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1153\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1154\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1326\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1327\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1328\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1329\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1330\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1332\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1333\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1334\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1335\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1336\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1317\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1318\u001b[0m       return self._call_tf_sessionrun(\n\u001b[0;32m-> 1319\u001b[0;31m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1320\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1321\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1405\u001b[0m     return tf_session.TF_SessionRun_wrapper(\n\u001b[1;32m   1406\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1407\u001b[0;31m         run_metadata)\n\u001b[0m\u001b[1;32m   1408\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1409\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_tf_sessionprun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "r = np.array(range(rel_total))\n",
    "h_t_pairs = combinations(range(ent_total),2)\n",
    "#all_score = np.array([])\n",
    "all_score = np.zeros([int(step_total),int(chunk_size)])\n",
    "\n",
    "c = 0\n",
    "while c < step_total:\n",
    "    start = time.time()\n",
    "    p = itertools.islice(h_t_pairs,0,chunk_size)\n",
    "    h,t = zip(*list(p))\n",
    "    h = np.array(h)\n",
    "    t= np.array(t)\n",
    "    \n",
    "    r_t = np.tile(r,h.size)\n",
    "    #h = h.repeat(rel_total)\n",
    "    #t = t.repeat(rel_total)\n",
    "    \n",
    "    #res = con.test_step(h.repeat(rel_total), t.repeat(rel_total), r_t).reshape(-1,rel_total)\n",
    "    res = con.test_step(h.repeat(rel_total), t.repeat(rel_total), r_t)\n",
    "    ind = res < 0\n",
    "    dis_avg = np.divide(np.sum(np.multiply(res,ind).reshape(-1,rel_total),1), np.sum(ind.reshape(-1,rel_total),1)+ 1e-9 ) \n",
    "    #all_score = np.append(all_score,dis_avg)\n",
    "    all_score[c,:] = dis_avg\n",
    "    c += 1\n",
    "    stop = time.time()\n",
    "    print(c , stop - start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`all_score` contains the score for all pairs. Now we go through them all to filter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-17T23:29:00.690519Z",
     "start_time": "2020-05-17T23:29:00.680489Z"
    }
   },
   "outputs": [],
   "source": [
    "all_score = all_score.reshape(-1)[:int(tot)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-17T23:28:33.546369Z",
     "start_time": "2020-05-17T23:28:33.459811Z"
    }
   },
   "outputs": [],
   "source": [
    "h = []\n",
    "t = []\n",
    "for a in zip(combinations(range(ent_total),2), all_score ):\n",
    "    if a[1] < DIST_AVG_THR:\n",
    "        h.append(a[0][0])\n",
    "        t.append(a[0][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-17T23:28:36.025405Z",
     "start_time": "2020-05-17T23:28:36.007206Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>head</th>\n",
       "      <th>tail</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>2358</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   head  tail\n",
       "0     0  2358"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d = {'head':h , 'tail': t}\n",
    "filt_dist_df = pd.DataFrame(data=d)\n",
    "filt_dist_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pass to append module"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Append module "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we provide the append function. The result from each one of the previous sections, is a data frame with two columns, _head_ and _tail_. That data frame must be appende to `train.txt` in the format of \n",
    "```\n",
    "head_mid /similar_to tail_mid\n",
    "```\n",
    "The following function is written to do exactly that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-13T02:44:01.126982Z",
     "start_time": "2020-05-13T02:44:01.117451Z"
    }
   },
   "outputs": [],
   "source": [
    "def append_train(input_df,new_name):\n",
    "    \"\"\" Appends the input data frame to a copy of train.txt.\n",
    "    \n",
    "    input_df: --pd.DataFrame: has two columns 'head', and 'tail' containing\n",
    "    the integer ids for heads and tails of similar tuples.\n",
    "    new_name: --str: name of the new file will be train_{new_name}.txt \n",
    "    \"\"\"\n",
    "    import datetime\n",
    "    new_name = new_name + str(datetime.datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\"))\n",
    "    dest = './FB15K/train_'+ new_name + '.txt'\n",
    "    while os.path.isfile(dest):\n",
    "        new_name = input(\"File already exists. Give another name: \")\n",
    "        dest = './FB15K/train_'+ new_name + '.txt'\n",
    "    \n",
    "    heads = list(input_df['head']) \n",
    "    tails = list(input_df['tail']) \n",
    "\n",
    "    # Translate int ids to /mid\n",
    "    ents = pd.read_csv(\"./OpenKE/benchmarks/FB15K/entity2id.txt\",sep = '\\t',header=None, skiprows=[0],usecols=[0]) # first row is lineTot\n",
    "    heads_mid = list(ents.iloc[heads,0]) \n",
    "    rels_mid = ['/similar_to']*len(heads)\n",
    "    tails_mid = list(ents.iloc[tails,0]) \n",
    "\n",
    "    d = {'head': heads_mid , 'relation': rels_mid, 'tail':tails_mid}\n",
    "    df = pd.DataFrame(data=d)\n",
    "\n",
    "    from shutil import copyfile\n",
    "    copyfile('./FB15K/train.txt', dest)\n",
    "    df.to_csv(dest, mode='a', header=False,index=False, sep='\\t')\n",
    "    return dest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-13T02:44:37.479146Z",
     "start_time": "2020-05-13T02:44:36.391298Z"
    }
   },
   "outputs": [],
   "source": [
    "# Create the enriched training files.\n",
    "# it append each df to training.txt and save with a unique name.\n",
    "# And put the name of all these new files in a list to be later used by AMIE\n",
    "\n",
    "train_file_name = [] # list of new enriched training files.\n",
    "if SUBJ:\n",
    "    train_sub = append_train(filt_sub_df, 'subj')\n",
    "    train_file_name.append(train_sub)\n",
    "if COS:    \n",
    "    train_cos = append_train(filt_cos_df, 'cos')\n",
    "    train_file_name.append(train_cos)\n",
    "if DIST_AVG:\n",
    "    train_dist = append_train(filt_dist_df, 'dist')\n",
    "    train_file_name.append(train_dist)\n",
    "\n",
    "    \n",
    "#train_file_name = [train_sub,train_cos,train_dist]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-13T02:44:38.932504Z",
     "start_time": "2020-05-13T02:44:38.925807Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['./FB15k/train_subj2020-05-12_22-44-36.txt',\n",
       " './FB15k/train_cos2020-05-12_22-44-37.txt',\n",
       " './FB15k/train_dist2020-05-12_22-44-37.txt']"
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_file_name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AMIE and Evaluations "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running AMIE on each training file produces two outputs. Firsts the rules `./rules/{KB_name}_rules.txt` and then the evaluation of the rules `./evaluation/{KB_name}_rules_eval.txt`. Then calling the function `eval_frame(\"./evaluation/{KB_name}_rules_eval.txt\")` will measure the accuracy of rules by Hits@10.\n",
    "\n",
    "After enriching the KB with similarity links, we run the above procedure twice. Once on `train.txt` to get __Baseline evaluations__ and again on `train_enriched_{name}.txt`. Then compare the outputs from `eval_frame()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-13T02:45:32.219986Z",
     "start_time": "2020-05-13T02:45:32.209024Z"
    }
   },
   "outputs": [],
   "source": [
    "def eval_frame(file, test_len):\n",
    "    \n",
    "    # Open file\n",
    "    f = open(file)\n",
    "    \n",
    "    # Hits counter\n",
    "    hits = 0\n",
    "    \n",
    "    # Loop though all facts in KB\n",
    "    for x in range(test_len):\n",
    "\n",
    "        # Read line\n",
    "        fact = f.readline()\n",
    "        fact = fact.split(' ')\n",
    "        if fact != ['']:\n",
    "            # Get target head and tail\n",
    "            head_target = fact[0]\n",
    "            tail_target = fact[2][:-1]\n",
    "\n",
    "\n",
    "            # Get head predictions\n",
    "            headpreds = f.readline()\n",
    "            headpreds = headpreds.split(' ')\n",
    "            headpreds = headpreds[1].split('\\t')\n",
    "            headpreds.pop()\n",
    "\n",
    "            # Get tail predictions\n",
    "            tailpreds = f.readline()\n",
    "            tailpreds = tailpreds.split(' ')\n",
    "            tailpreds = tailpreds[1].split('\\t')\n",
    "            tailpreds.pop()\n",
    "\n",
    "\n",
    "            if (head_target in headpreds) and (tail_target in tailpreds):\n",
    "                if (len(headpreds) < 10) and (len(tailpreds) < 10):\n",
    "                    hits+=1\n",
    "        else:\n",
    "            print('miss')\n",
    "                \n",
    "    return hits/(test_len)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline evaluation "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-11T17:28:01.208400Z",
     "start_time": "2020-05-11T17:28:01.202508Z"
    }
   },
   "source": [
    "Just type in the right files in the next cell and continue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-13T02:45:35.587466Z",
     "start_time": "2020-05-13T02:45:35.582619Z"
    }
   },
   "outputs": [],
   "source": [
    "train_add = \"./FB15K/train.txt\"\n",
    "test_add = \"FB15K/test.txt\"\n",
    "valid_add = \"FB15K/valid.txt\"\n",
    "\n",
    "rules_add = \"rules/baseline_rules.txt\"\n",
    "eval_add = \"evaluation/baseline_rules_eval.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-13T02:45:40.344364Z",
     "start_time": "2020-05-13T02:45:40.338809Z"
    }
   },
   "outputs": [],
   "source": [
    "# The text of the commands for running AMIE\n",
    "\n",
    "AMIE_plus = (\"java -XX:-UseGCOverheadLimit -Xmx64g -jar AMIE/amie_plus.jar \"\n",
    "\"-minhc 0.25 -mins 50 -minis 0 \" \n",
    "f\"{train_add} > {rules_add}\")\n",
    "\n",
    "Apply_AMIE_RULES = (f'java -jar AMIE/ApplyAMIERules.jar {rules_add}' \n",
    "                    f' {train_add} {test_add} {valid_add}'\n",
    "                    f' {eval_add}')\n",
    "\n",
    "if not os.path.exists('./rules'):\n",
    "    os.mkdir('./rules')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-13T02:45:44.475917Z",
     "start_time": "2020-05-13T02:45:44.467066Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'java -jar AMIE/ApplyAMIERules.jar rules/baseline_rules.txt ./FB15k/train.txt FB15k/test.txt FB15k/valid.txt evaluation/baseline_rules_eval.txt'"
      ]
     },
     "execution_count": 181,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# AMIE_plus\n",
    "Apply_AMIE_RULES"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next cell will generate the rules and save them in `rules_add`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-13T00:47:01.824392Z",
     "start_time": "2020-05-13T00:14:36.457Z"
    }
   },
   "outputs": [],
   "source": [
    "os.system(AMIE_plus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clean the output from previous cell (i.e. `rules_add`) before running applying AMIE rules. The header and footer of the file `rules_add` must be deleted. It should only contain rules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-13T02:45:52.805319Z",
     "start_time": "2020-05-13T02:45:52.762666Z"
    }
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'rules/baseline_rules.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-182-259bc1997855>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Rules at %s file cleaned.'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m \u001b[0mclean_amie_output\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrules_add\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-182-259bc1997855>\u001b[0m in \u001b[0;36mclean_amie_output\u001b[0;34m(path)\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mWarning\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mthis\u001b[0m \u001b[0mfunction\u001b[0m \u001b[0moverwrites\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mfile\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \"\"\"\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m         \u001b[0mf_contents\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadlines\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'rules/baseline_rules.txt'"
     ]
    }
   ],
   "source": [
    "def clean_amie_output(path):\n",
    "    \"\"\"\n",
    "    Warning: this function overwrites the file in path\n",
    "    \"\"\"\n",
    "    with open(path, 'r') as f:\n",
    "        f_contents = f.readlines()\n",
    "        \n",
    "    f_contents = f_contents[13:-3]\n",
    "\n",
    "    with open(path, 'w') as f:\n",
    "        f.writelines(f_contents)\n",
    "        \n",
    "    print('Rules at %s file cleaned.' % path)\n",
    "\n",
    "clean_amie_output(rules_add)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-13T02:46:26.155895Z",
     "start_time": "2020-05-13T02:46:21.651066Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "33280"
      ]
     },
     "execution_count": 183,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if not os.path.exists('./evaluation'):\n",
    "    os.mkdir('./evaluation')\n",
    "\n",
    "os.system(Apply_AMIE_RULES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-13T02:46:29.476024Z",
     "start_time": "2020-05-13T02:46:29.414126Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "59071"
      ]
     },
     "execution_count": 184,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get the lenght of the test file. It is fed to eval_frame()\n",
    "import subprocess\n",
    "test_len = subprocess.run(['wc', '-l', test_add], stdout=subprocess.PIPE).stdout.decode('utf-8')\n",
    "test_len = int(test_len.split()[0])\n",
    "test_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-13T00:47:01.835299Z",
     "start_time": "2020-05-13T00:14:36.474Z"
    }
   },
   "outputs": [],
   "source": [
    "print(eval_add)\n",
    "print('Hits@10: ' + str(eval_frame(eval_add, test_len)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Enriched KB Evaluation "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Basically repeat everything from Baseline Evaluation for the enriched training file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-13T02:46:35.699110Z",
     "start_time": "2020-05-13T02:46:35.693868Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_subj2020-05-12_22-44-36.txt\n",
      "_cos2020-05-12_22-44-37.txt\n",
      "_dist2020-05-12_22-44-37.txt\n"
     ]
    }
   ],
   "source": [
    "# Enriched training file names:\n",
    "for name in train_file_name:\n",
    "    print(name[13:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-13T02:59:09.826337Z",
     "start_time": "2020-05-13T02:57:47.975810Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The enriched tr file: FB15k/train_subj2020-05-12_22-44-36.txt\n",
      "Rules will be saved at: rules/Enriched_rules_subj2020-05-12_22-44-36.txt\n",
      "And rule evaluations at: evaluation/Enriched_eval_subj2020-05-12_22-44-36.txt\n",
      "\n",
      " AMIE_plus output: 33280\n",
      "Rules at rules/Enriched_rules_subj2020-05-12_22-44-36.txt file cleaned.\n",
      "\n",
      " Apply_AMIE_Rules output: 0\n",
      "\n",
      " Hits@10: 0.03925784225762218\n",
      "\n",
      "\n",
      "The enriched tr file: FB15k/train_cos2020-05-12_22-44-37.txt\n",
      "Rules will be saved at: rules/Enriched_rules_cos2020-05-12_22-44-37.txt\n",
      "And rule evaluations at: evaluation/Enriched_eval_cos2020-05-12_22-44-37.txt\n",
      "\n",
      " AMIE_plus output: 33280\n",
      "Rules at rules/Enriched_rules_cos2020-05-12_22-44-37.txt file cleaned.\n",
      "\n",
      " Apply_AMIE_Rules output: 0\n",
      "\n",
      " Hits@10: 0.0011172995209155084\n",
      "\n",
      "\n",
      "The enriched tr file: FB15k/train_dist2020-05-12_22-44-37.txt\n",
      "Rules will be saved at: rules/Enriched_rules_dist2020-05-12_22-44-37.txt\n",
      "And rule evaluations at: evaluation/Enriched_eval_dist2020-05-12_22-44-37.txt\n",
      "\n",
      " AMIE_plus output: 33280\n",
      "Rules at rules/Enriched_rules_dist2020-05-12_22-44-37.txt file cleaned.\n",
      "\n",
      " Apply_AMIE_Rules output: 0\n",
      "\n",
      " Hits@10: 0.0\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# For each enriched training file, apply AMIE and show its HIT@10 performance\n",
    "for name in train_file_name:\n",
    "    train_add = \"FB15K/train\" +  name[13:] # From append module\n",
    "    rules_add = \"rules/Enriched_rules\" + name[13:] # modify this name if you like\n",
    "    eval_add = \"evaluation/Enriched_eval\" + name[13:] # same here\n",
    "\n",
    "    test_add = \"FB15K/test.txt\"\n",
    "    valid_add = \"FB15K/valid.txt\"\n",
    "\n",
    "    print(\"The enriched tr file: \" + train_add)\n",
    "    print(\"Rules will be saved at: \"+ rules_add)\n",
    "    print(\"And rule evaluations at: \" + eval_add)\n",
    "    \n",
    "    # The texts of the commands for running AMIE\n",
    "    AMIE_plus = (\"java -XX:-UseGCOverheadLimit -Xmx4g -jar AMIE/amie_plus.jar \"\n",
    "    \"-minhc 0.0 -mins 0 -minis 0 \" \n",
    "    f\"{train_add} > {rules_add}\")\n",
    "\n",
    "    Apply_AMIE_RULES = (f'java -jar AMIE/ApplyAMIERules.jar {rules_add}' \n",
    "                        f' {train_add} {test_add} {valid_add}'\n",
    "                        f' {eval_add}')\n",
    "\n",
    "    x = os.system(AMIE_plus)\n",
    "    print(\"\\n AMIE_plus output: \" + str(x))\n",
    "    \n",
    "    # trim `Enriched_rules{}.txt` again\n",
    "    clean_amie_output(rules_add)\n",
    "\n",
    "    y = os.system(Apply_AMIE_RULES) # if output is 256 then you forgot to trim\n",
    "    print(\"\\n Apply_AMIE_Rules output: \" + str(y))\n",
    "    \n",
    "    print('\\n Hits@10: ' + str(eval_frame(eval_add, test_len)))\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "204.8px"
   },
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "position": {
    "height": "246.85px",
    "left": "654px",
    "right": "20px",
    "top": "119px",
    "width": "350px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
